import concurrent
import concurrent.futures
from os import cpu_count
from pathlib import Path
from typing import Callable, List, Dict, Optional, Tuple, Iterator, Literal
from warnings import warn
from tqdm.notebook import tqdm
import random

MODES = ('one_input', 'zip', 'modulo', 'custom')


class ProcessingStep:
    def __init__(self,
                 name: str,
                 process_function: Callable,
                 input_dirs: Optional[str | Path | List[str | Path]] = None,
                 output_dirs: Optional[str | Path | List[str | Path]] = None,
                 pairing_method: Literal[*MODES] = 'one_input',  # type: ignore
                 pairing_function: Optional[Callable[[List[List[Path]]], Iterator[Tuple]]] = None,
                 fixed_input: bool = False,
                 root_dir: Optional[str | Path] = None,
                 sample_k: Optional[int] = None,
                 workers: Optional[int] = 1,
                 options: Optional[Dict] = None):
        """
        TODO: rewrite et uniformiser les styles de docstring (numpy ou Google)
        Initialise une √©tape de traitement g√©n√©rique.

        Args:
            name (str): Nom lisible de l'√©tape.
            process_function (Callable): La fonction qui effectue le traitement.
                Signature attendue : (*input_paths: Path, output_paths: List[Path], **options) -> Optional[Path | List[Path]]
                Doit accepter un nombre variable d'arguments Path en entr√©e (selon le mode),
                la liste des chemins de sortie, et les options.
                Doit retourner le(s) chemin(s) du/des fichier(s) sauvegard√©(s), ou None si √©chec/rien √† sauver.
            input_dirs (List): Liste des chemins des dossiers d'entr√©e (relatifs ou absolus).
            output_dirs (List): Liste des chemins des dossiers de sortie (relatifs ou absolus).
            pairing_method (PairingMethod): Comment combiner les fichiers des input_dirs.
                Options: 'one_input' (d√©faut), 'zip', 'modulo', 'custom'.
            pairing_function (Callable): Requis si method='custom'. Voir doc _generate_processing_args.
            fixed_input (Bool): TODO: ajouter description d√©j√† √©crite ailleurs...
                                TODO 2 : prendre en charge le fixed_input avec les listes de dossiers -> liste de bool ? oO
            root_dir (Optional): Dossier racine pour r√©soudre les chemins relatifs.
            options (Optional[Dict]): Arguments (kwargs) additionnels pass√©s √† process_function.
        """
        # TODO: accepter le nom d'une √©tape lors des manipulations (insertions, ...)
        self.name = name 
        self.process_function = process_function
        # TODO : Tester None au lieu de Path('.') ou m√™me cwd() probablement encore mieux
        self.root_dir = Path(root_dir) if root_dir else None 
        self.process_kwargs = options or {}
        self.sample_k = sample_k

        # R√©solution des chemins
        self.input_paths: List[Path] = self._resolve_paths(input_dirs or [])
        self.output_paths: List[Path] = self._resolve_paths(output_dirs or [])
        # TODO: si output_dir non rempli, cr√©er un dossier du m√™me nom que l'√©tape ? 
        # => v√©rification d'accents et replace les ` ` par `_`
        self.fixed_input = fixed_input

        if not self.output_paths:
            raise ValueError(f"L'√©tape '{self.name}' doit avoir au moins un 'output_dirs' d√©fini.") 
            # et bah pas forc√©ment ! si on √©crase le fichier d'input ! ->  on donne l'output = input donc pas vide ?
            # TODO: juste enlever ce check ou y'a d'autres implications ?

        # Validation du mode (s√©curit√© runtime) -> k√©sako ?
        # NOTE: Le type Literal fait d√©j√† une v√©rification statique ‚Üí statique = ? ouvrir un dico...
        if pairing_method not in set(MODES): 
            raise ValueError(f"Mode d'appariement' '{pairing_method}' invalide. Choisir parmi: {MODES}")
        # On laisse pour avenir lointain
        if pairing_method == 'custom' and not callable(pairing_function):
            raise ValueError("Une `pairing_function` valide est requise pour le mode 'custom'.")
        self.pairing_method = pairing_method
        self.pairing_function = pairing_function

        # Map pour suivre les sorties g√©n√©r√©es par entr√©e(s)
        self.processed_files_map: Dict[Tuple[Path, ...], Path | List[Path]] = {}  # Cl√© est tuple de Path d'entr√©e

        # Gestion de la parall√©lisation
        nb_cpus = cpu_count()
        if workers > nb_cpus:
            warn(f"Nombre de workers parall√®les ajust√© √† {nb_cpus} (maximum syst√®me).")
        if workers == -1:
            workers = nb_cpus
        self.parallels_workers = min(workers, nb_cpus)

    def _resolve_paths(self, dir_list: str | Path | List[str | Path]) -> List[Path]:
        """Convertit et r√©sout les chemins par rapport au root_dir. 
        Chaque chemin de la liste est converti en Path.
        Si un chemin n'est pas absolu, il est consid√©r√© relatif au dossier racine.
        """
        # assert dans une liste
        dir_list = [dir_list] if not isinstance(dir_list, list) else dir_list

        resolved = []
        for folder in dir_list:
            if not isinstance(folder, (str, Path)):
                raise ValueError(f"un √©l√©ment ne repr√©sente pas un dossier ou un chemin : {folder}")
            
            dir_path = Path(folder)  # Path(Path()) pose pas de probl√®me
            # Si le chemin n'est pas absolu, on le consid√®re relatif au root_dir
            if not dir_path.is_absolute() and self.root_dir:
                resolved.append(self.root_dir / dir_path)
            else:
                resolved.append(dir_path)
        return resolved

    # TODO: Impl√©menter __str__ pour un r√©sum√© utile de l'√©tape (inputs, outputs, mode)
    def __str__(self) -> str:
        input_str = ", ".join([p.name for p in self.input_paths])
        output_str = ", ".join([p.name for p in self.output_paths])
        return (f"√âtape '{self.name}':\n"
                f"  Entr√©e(s) : [{input_str}] (Mode: {self.pairing_method})\n"
                f"  Sortie(s) : [{output_str}]\n"
                f"  Options   : {self.process_kwargs}")

    def _get_files_from_inputs(self) -> List[List[Path]]:
        """Liste les fichiers de chaque dossier d'entr√©e. L√®ve une erreur si un dossier n'existe pas."""
        all_file_lists = []
        if not self.input_paths:
            raise ValueError(f"{self.name} : Aucun dossier d'entr√©e d√©fini.")

        print(f"Info [{self.name}]: R√©cup√©ration des chemins de fichiers d'entr√©e...")
        for input_dir in self.input_paths:
            if not input_dir.is_dir():
                # Lever une erreur si le dossier n'existe pas (ou n'est pas un dossier)
                raise FileNotFoundError(f"Le dossier d'entr√©e sp√©cifi√© n'existe pas: '{input_dir}' pour l'√©tape '{self.name}'")
            
            # TODO: ce try serait mieux catch en amont et directement raise des erreur
            try:
                # Lister tous les fichiers et trier
                files = sorted([f for f in input_dir.iterdir() if f.is_file()])
                # TODO : fonction utilitaire pour g√©rer les pluriel. (ou lib "inflect")
                print(f"  '{input_dir.name}' : {len(files)} fichiers trouv√©s.") 
                all_file_lists.append(files)
            except Exception as e:
                # FIXME: horrible : exception (tout) renvoie une IOError
                # G√©rer autres erreurs potentielles (ex: permissions)
                raise IOError(f"√âchec de l'inventaire du dossier {input_dir}") from e

        return all_file_lists

    def _generate_processing_inputs(self, input_file_lists: List[List[Path]]) -> Iterator[Tuple[Path, ...]]:
        """
        G√©n√®re les tuples d'arguments (chemins) pour `process_function` bas√© sur le mode.
        
        TODO : lister les modes disponibles avec courte description de la logique

        Args:
            input_file_lists: Liste contenant des listes de Path, pour chaque dossier d'entr√©e.

        Yields:
            Tuple[Path, ...]: Un tuple de chemins (Path) √† passer comme *args
                              √† la fonction de traitement pour chaque appel.
                              *chaque √©l√©ment du tuple est "d√©paquet√©" et repr√©sente un argument dans la fonction attendue*
                              *ne peut pas √™tre d√©clar√© avec un nom d'argument*
                                def foo(*args): OK
                                def foo(bar=*args) Error
        """
        input_len = len(input_file_lists)

        # V√©rifier qu'aucune liste n'est vide (zip s'arr√™terait, mais c'est plus clair de pr√©venir)
        if not all(input_file_lists):
            empty_folders = [str(self.input_paths[i]) for i, lst in enumerate(input_file_lists) if not lst] 
            raise FileNotFoundError(f"Aucun fichier trouv√© dans les dossiers d'entr√©e {empty_folders} pour l'√©tape '{self.name}'.")
        
        # Pr√©l√®ve le nombre d'√©l√©ments prescrit. Aux m√™me ids pour chaque liste d'input
        if self.sample_k and isinstance(self.sample_k, int):
            sample_ids = random.sample(range(len(input_file_lists[0])), self.sample_k)
            input_file_lists = [[file_list[i] for i in sample_ids] for file_list in input_file_lists]
        
        # Mode de g√©n√©ration :
        if self.pairing_method == 'one_input':
            if input_len == 0:  # S√©curit√©
                raise ValueError("Mode 'one_input' mais aucun dossier d'entr√©e fourni.")
            input_files = input_file_lists[0]

            for file_path in input_files:
                yield (file_path,)  # Tuple avec un seul √©l√©ment

        elif self.pairing_method == 'zip':
            if input_len < 2:
                raise ValueError("Le mode 'zip' requiert au moins 2 dossiers d'entr√©e.")

            yield from zip(*input_file_lists)

        elif self.pairing_method == 'modulo':
            # √† la diff√©rence de zip, modulo revient au d√©but de la deuxi√®me liste si elle est totalement parcourue
            if input_len != 2:
                raise ValueError("Le mode 'modulo' requiert exactement 2 dossiers d'entr√©e.")
            list1 = input_file_lists[0]
            list2 = input_file_lists[1]

            # TODO ? shuffle_input en option ? pareil pour zip ?
            # shuffle seulement la 2e liste (backgrounds) suffisant.
            random.shuffle(list2)

            list2_len = len(list2)
            for i, path1 in enumerate(list1):
                path2 = list2[i % list2_len]
                yield (path1, path2)
        
        elif self.pairing_method == 'sampling':
            # m√©thode sp√©cifique pour l'√©tape de transformation
            # TODO: √† g√©n√©raliser ?

            input_files = input_file_lists[0]
            input_labels = input_file_lists[1]

            # Sample un set des fichiers o√π appliquer la transfo
            blur_sample = set(random.sample(input_files, int(len(input_files)*0.3)))
            # Cr√©e une liste de bool√©ens (donn√©s en param√®tres d'input) 
            # Indiquant si l'√©l√©ment √©valu√© doit subir la transformation
            do_blur = [i in blur_sample for i in input_files]

            # idem pour la transfo RGB
            rgb_sample = set(random.sample(input_files, int(len(input_files)*0.3)))
            do_rgb = [i in rgb_sample for i in input_files]

            yield from zip(input_files, input_labels, do_blur, do_rgb)

        
        elif self.pairing_method == 'custom':
            if not self.pairing_function:
                raise ValueError("Fonction `pairing_function` manquante pour le mode 'custom'.")
            
            yield from self.pairing_function(input_file_lists)

        else:
            # Normalement impossible gr√¢ce √† Literal et la v√©rif init
            raise NotImplementedError(f"Mode d'appariement' '{self.pairing_method}' non impl√©ment√©e.")

    def run(self):
        """Ex√©cute l'√©tape de traitement pour tous les √©l√©ments/paires d'entr√©e."""
        self.processed_files_map = {}  # Retrace les r√©sultats, y'a un truc √† faire avec... un jour...
        print(f"--- Ex√©cution √âtape : {self.name} ---")
        # print(self) # Utiliser __str__ pour afficher les d√©tails si besoin 
        # TODO : param√®tre verbose

        # 1. Cr√©er les dossiers de sortie (une seule fois au d√©but)
        print(f"Info [{self.name}]: V√©rification/Cr√©ation des dossiers de sortie...")
        for output_path in self.output_paths:
            try:
                output_path.mkdir(parents=True, exist_ok=True)
                print(f"  Sortie -> '{output_path}'")
            except IOError as ioe:
                raise IOError(f"Impossible de cr√©er le dossier de sortie '{output_path}': {ioe}") from ioe 
            except Exception as e:
                print(f"Erreur lors de la cr√©ation du dossier {output_path}. {e}")
                return
        
        # 2. Lister les fichiers d'entr√©e
        try:
            input_file_lists = self._get_files_from_inputs()
        except (FileNotFoundError, ValueError, IOError) as e:
            print(f"Erreur [{self.name}]: Condition pr√©alable non remplie pour d√©marrer l'√©tape. {e}")
            return

        # 3. Obtenir l'it√©rateur d'arguments
        try:
            argument_iterator = self._generate_processing_inputs(input_file_lists)
            # TODO et si le mode du g√©n√©rateur renvoyait un tuple avec le total d'op√©ration ? (pout tqdm ts√© üëÄ) 
            # => solution : cr√©er une classe g√©n√©rator qui yield comme la m√©thode et poss√®de un attribut .total
        except (ValueError, NotImplementedError) as e:
            print(f"Erreur [{self.name}]: Impossible de g√©n√©rer les arguments pour le mode '{self.pairing_method}'. {e}")
            return

        # Calcul du total pour tqdm
        total_items = None
        try:
            if self.pairing_method == 'one_input': total_items = len(input_file_lists[0])
            elif self.pairing_method == 'modulo': total_items = len(input_file_lists[0])
            elif self.pairing_method == 'zip': total_items = min(len(lst) for lst in input_file_lists if lst)
            else: raise ValueError(f"mode d'appariemment inconnu, utiliser un parmi {MODES}")
        except Exception: total_items = None

        # --------------------------------------------------------------------------------------------
        #                    4. Boucle de traitement (avec tqdm SoonTM tkt)
        # --------------------------------------------------------------------------------------------

        processed_count, errors_count = self._processing_loop(argument_iterator, total_items)

        # TODO: int√©grer le timings (quoique, avec tqdm.... :pray:)
        print(f"--- √âtape {self.name} termin√©e ---") 
        print(f"  {processed_count} √©l√©ments trait√©s avec succ√®s (fichiers de sortie g√©n√©r√©s).")
        if errors_count > 0:
            print(f"  {errors_count} erreur(s) ou traitement(s) sans retour.")

    def _processing_loop(self, 
                         argument_iterator: Iterator[Tuple[Path, ...]], 
                         total_items: int) -> Tuple[int, int]:
        """Ex√©cute la boucle de traitement principale, soit en s√©quentiel, soit en parall√®le.
        Met √† jour self.processed_files_map et retourne les compteurs.
        """
        processed_count = 0
        error_count = 0

        # --- Logique S√©quentielle ---
        if not self.parallels_workers or 0 <= self.parallels_workers <= 1:
            print(f"Info [{self.name}]: Ex√©cution en mode s√©quentiel...")
            for input_args_tuple in tqdm(argument_iterator, 
                                         desc=self.name, 
                                         total=total_items, 
                                         unit="item", 
                                         leave=True, 
                                         smoothing=0):
                try:
                    # Cl√© pour le suivi
                    input_key = input_args_tuple

                    # Appel de la fonction de traitement
                    saved_output_paths: Optional[Path | List[Path]] = self.process_function(
                        *input_args_tuple,              # D√©paquette les chemins d'entr√©e
                        output_dirs=self.output_paths,  # liste des dossiers de sortie
                        **self.process_kwargs           # Passage des options en kwargs
                    )

                    # V√©rification du retour de la fonction de traitement
                    if saved_output_paths:
                        # on v√©rifie qu'on a bien un type Path (ou liste de paths)
                        # utilit√© ? 
                        if isinstance(saved_output_paths, Path) or \
                           (isinstance(saved_output_paths, list) and all(isinstance(p, Path) for p in saved_output_paths)):
                            self.processed_files_map[input_key] = saved_output_paths
                            processed_count += 1
                        else:
                            # TODO: utiliser des vrais warn ou syst√®me de logging
                            tqdm.write(f"Avertissement [{self.name}]: Retour invalide de {self.process_function} pour {input_key} (type: {type(saved_output_paths)}).")
                            error_count += 1
                    else:
                        # la fonction a retourn√© None (√©chec g√©r√© ou rien √† sauvegarder)
                        error_count += 1
                        # Option : logger l'entr√©e qui n'a rien produit
                except Exception as e_proc:
                    # Erreur innatendue dans process_function ou lors de l'appel
                    tqdm.write(f"\nErreur [{self.name}]: √âchec traitement de {input_args_tuple}: {e_proc}")
                    error_count += 1
            return processed_count, error_count
        
        # --- Logique Parall√®le --- 
        elif self.parallels_workers > 1 or self.parallels_workers == -1:
            print(f"Info [{self.name}]: Ex√©cution en mode parall√®le avec {self.parallels_workers} workers...")

            list_of_input_args = list(argument_iterator)
            if not list_of_input_args:
                raise RuntimeError(f"Aucun argument √† traiter apr√®s g√©n√©ration. Fin.")
            
            # Mettre √† jour total_item si l'it√©rateur a √©t√© consom√©
            if total_items is None:
                total_items = len(list_of_input_args)
            
            with concurrent.futures.ProcessPoolExecutor(max_workers=self.parallels_workers) as executor:
                # Dictionnaire pour mapper les futures aux arguments d'entr√©e (pour le logging d'erreur)
                future_to_args: Dict[concurrent.futures.Future, Tuple[Path, ...]] = {}

                print(f"Info [{self.name}]: Soumission de {len(list_of_input_args)} t√¢ches au pool de processus...")
                for input_args_tuple in list_of_input_args:
                    try:
                        future = executor.submit(
                            self.process_function,
                            *input_args_tuple,
                            output_dirs=self.output_paths,
                            **self.process_kwargs
                        )
                        future_to_args[future] = input_args_tuple
                    except Exception as e_submit:
                        tqdm.write(f"Erreur [{self.name}]: √âchec de la soumission de la t√¢che pour {input_args_tuple}: {e_submit}")
                        error_count += 1
                
                for future in tqdm(concurrent.futures.as_completed(future_to_args.keys()),
                                   total=len(future_to_args),
                                   desc=self.name,
                                   unit="item",
                                   leave=True,
                                   smoothing=0):
                    # R√©cup√®re les args d'origine pour ce future
                    input_key = future_to_args[future] 

                    try:
                        saved_output_paths: Optional[Path | List[Path]] = future.result() # Bloque jusqu'√† r√©sultat

                        if saved_output_paths:
                            if isinstance(saved_output_paths, Path) or \
                               (isinstance(saved_output_paths, list) and all(isinstance(p, Path) for p in saved_output_paths)):
                                self.processed_files_map[input_key] = saved_output_paths
                                processed_count += 1
                            else:
                                tqdm.write(f"Avertissement [{self.name}]: Retour invalide (parall√®le) pour {input_key} (type: {type(saved_output_paths)}).")
                                errors_count += 1
                        else:
                            errors_count += 1
                    except Exception as e_exec: # Erreur DANS le processus enfant
                        tqdm.write(f"\nErreur [{self.name}]: √âchec t√¢che parall√®le pour {input_key}: {e_exec}")
                        # import traceback; tqdm.write(traceback.format_exc()) # Pour debug
                        errors_count += 1
            return processed_count, error_count
        
        else:
            raise ValueError(f"")
            

class ProcessingPipeline:
    def __init__(self, root_dir: Optional[str | Path] = None):
        self.steps: List[ProcessingStep] = []
        # d√©finit le dossier source du pipeline ‚Üí obligatoire ?
        self.root_dir = Path(root_dir) if root_dir else None  # Path.cwd() ?

    def add_step(self, step: ProcessingStep, position=None):
        if not self.steps and not step.input_paths: 
            # step.input_paths n'est jamais None, au minimum []
            raise ValueError(f"La premi√®re √©tape ('{step.name}') doit avoir `input_dirs` d√©finie.")

        # Si un dossier racine est d√©fini dans le pipeline, mais pas dans l'√©tape :
        # il est transmis √† l'√©tape d√®s son ajout sans l'√©craser
        if self.root_dir and not step.root_dir:
            step.root_dir = self.root_dir
            # r√©√©value les chemins suite √† la modification (√©ventuelle) du root_dir
            step.input_paths = step._resolve_paths(step.input_paths)
            step.output_paths = step._resolve_paths(step.output_paths)

        # Si pas pr√©cis√©, on assert position √† la derni√®re √©tape (volontairement = len(steps) donc out of index)
        position = len(self.steps) if position is None else position
        
        # Si l'√©tape ajout√©e n'a pas d'input, on a forc√©ment au moins une √©tape dans le pipeline,
        # on veut chainer les outputs pr√©c√©dents dans l'input actuel
        if not step.input_paths:
            if position == 0:
                raise IndexError(f"Insertion en premi√®re position, impossible de d√©duire les dossiers d'input pour {step.name}")
            
            # Tous les cas probl√©matiques tombent dans un 'out of index' ou sont exclus par le check pos==0
            try:
                previous_step: ProcessingStep = self.steps[position - 1]
                # tant que l'√©tape est ajout√©e √† un autre point que la fin, on aura forc√©ment une √©tape suivante
                next_step: ProcessingStep = self.steps[position] if position < len(self.steps) else None

                # === CHAINAGE ===
                # si on arrive ici, toutes les exceptions sont d√©j√† √©lev√©es. Pas de risque de modification du pipeline malgr√© un blocage ensuite
                step.input_paths = previous_step.output_paths
                if next_step and not next_step.fixed_input:
                    # TODO : g√©rer les fixed_input en cas de len(input) > 1
                    next_step.input_paths = step.output_paths

            except IndexError as idx:
                raise ValueError(f"Position d'insertion invalide pour d√©duire les dossiers d'input de {step.name}.") from idx
            except Exception as e:
                raise RuntimeError(f"Erreur inattendue pour l'ajout de {step.name}") from e
        
        else:
            # on rentre si c'est la premi√®re √©tape (input_dirs *est* d√©fini)
            pass

        # === AJOUT DE L'√âTAPE ===
        self.steps.insert(position, step)

    def run(self, from_step_index: int = 0, only_one: bool = False):
        # TODO: v√©rifier si un seul des dossiers d'output des √©tapes √† run n'est pas vide => ne run pas
        # √©vite les runs par accident
        # cette v√©rification ne sera pas faite sur les step.run() pour permettre d'√©craser
        if from_step_index < 0 or from_step_index >= len(self.steps):
            raise IndexError(f"Invalid start index {from_step_index}. Pipeline has {len(self.steps)} steps.")
        
        steps_to_do = [self.steps[from_step_index]] if only_one else self.steps[from_step_index:]
        
        for i, step in enumerate(steps_to_do, start=from_step_index):
            print(f"Running √©tape {i}: {step.name}")
            step.run()
